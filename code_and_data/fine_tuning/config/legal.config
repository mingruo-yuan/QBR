[data]
train_data = ./data/train/train_data_all_50739.jsonl
test_data =  ./data/test/test_data_sample_top1_1000.json

[test]
pos_score = 1
k_list = 1,3,5
metric_list = MAP, MRR, R@k,P@k

test_baseline = False
baseline_ids = 0

test_ours = True
test_specific = None


[encoder]
backbone = mpnet
shared = True
pooling = avg
embedding_size = 128


[train]
checkpoint = None

epoch = 5
evidence_sample_num = 2

save_step = 1000
logging_step = 2000

batch_size = 1
sub_batch_size = 32

optimizer = adamw
grad_accumulate = 1
learning_rate = 1e-5
weight_decay = 0
step_size = 1
lr_multiplier = 1
reader_num = 1
fp16 = False
multi_gpu = False


[simcse_loss]
use = False
negatives_parallel = True
negatives_cross = False
negatives_parallel_single = False
sim_fct = cos
temperature = 0.1

[attention_loss]
use = False
separate_attention_peak = False


[contra_loss]
use = True
rm_simcse = False
unsupervise = False

negatives_attention = False
remove_hard_attention = False
positive_attention = False

positive_query = False
negatives_query = False

remove_hard_query = False
query = fact
neg_query_key = single


value_sample_num = 2
negatives_value = True
neg_value_key = single


sim_fct = cos
temperature = 0.1


[attention]
type = dot
scale = 1.0
temperature = 0.1


[positive_weight]
use = False
range = in_batch
normalize = none

source = dot
type = norm
log_sum = False


[output]
output_time = 1
test_time = 1

model_path = ./output/december/mpnet_only_supervise_label_50739


[baseline]
pooling = avg

model0 = mpnet
model1 = bert
model2 = bert-tiny
model3 = ance
model4 = roberta
model5 = dpr
model6 = tas-b
model7 = sbert

model8 = legal-simcse
model10 = tfidf
model11 = bm25
model12 = boe
